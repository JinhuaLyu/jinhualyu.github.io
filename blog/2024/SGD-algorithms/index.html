<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Stochastic Gradient Descent, SGD with momentum, AdaGrad, RMSProp, and Adam algorithms | Jinhua Lyu </title> <meta name="author" content="Jinhua Lyu"> <meta name="description" content="A discussion on different stochastic gradient algorithms and why they are efficient"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.css" integrity="sha256-VwMV//xgBPDyRFVSOshhRhzJRDyBmIACniLPpeXNUdc=" crossorigin="anonymous"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%98%98%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jinhualyu.github.io/blog/2024/SGD-algorithms/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jinhua</span> Lyu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li id="show-cv" class="nav-item"> <a class="nav-link" target="_blank" href="https://jinhualyu.github.io/assets/pdf/CV_Jinhua%20Lyu.pdf"> CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Stochastic Gradient Descent, SGD with momentum, AdaGrad, RMSProp, and Adam algorithms</h1> <p class="post-meta"> Created in June 18, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> optimization</a>   <a href="/blog/tag/algorithms"> <i class="fa-solid fa-hashtag fa-sm"></i> algorithms</a>   ·   <a href="/blog/category/sample-posts"> <i class="fa-solid fa-tag fa-sm"></i> sample-posts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As we know, the Adam algorithm is a highly efficient algorithm for solving unconstrained optimization problems, widely used in machine learning and deep learning. How does this algorithm work? What’s the logic behind it? Why is it so efficient? In this post, I would like to discuss the Adam algorithm and the core idea behind it.</p> <p>Since Adam is based on Stochastic Gradient Descent (SGD), it is necessary to first discuss some SGD algorithms. To delve deep into SGD algorithms, we’d better start with Gradient Descent (GD) algorithm, which is the fundamental algorithm in solving unconstrained optimization problems.</p> <h3 id="gd-algorithm">GD Algorithm</h3> <p>Suppose we have a general unconstrained optimization problem:</p> \[\min_{\theta} f(\theta)\] <p>Our goal is to find the optimal $\theta$ that minimizes the objective function. Unlike simple quadratic functions, most nonlinear functions are difficult to minimize directly by using basic mathematical techniques. GD is an effective and brilliant algorithm for solving such problems. The core idea of this algorithm is to iteratively adjust the variables incrementally. Many iterations are typically required to approach the optimal solution. In each iteration, we use a specific rule to update $\theta$:</p> \[\theta_{t+1} = \theta_t - \alpha \nabla f(\theta_t)\] <p>Here, $t$ represents the iteration index, $\theta$ is the variable being optimized, $\alpha$ is the step size determining how large a step should be taken in the opposite direction of the gradient, and $\nabla f$ is the gradient of the objective function. So why does this rule work? Acctually, this rule guarantees that the objective function decreases at each iteration.</p> <p>Suppose our goal if to find a rule that can improve the variable incrementally. At each iteration, we want to update it by:</p> \[\theta_{t+1} = \theta_t + \alpha p_t\] <p>where $p_t$ is a direction we pick at iteration t. At previous iteration, the objective value is $f(\theta_t)$, while at this iteration the objective value is $f(\theta_{t+1})$.</p> \[f(\theta_{t+1}) = f(\theta_t + \alpha p_t)\] <p>The objective value decreases if \(f(\theta_{t+1}) &lt; f(\theta_t)\), which implies $ f(\theta_t + \alpha p_t) &lt; f(\theta_t) $ for sufficiently small $\alpha &gt; 0$. Let us define $\phi : \mathbb{R}^+ \rightarrow \mathbb{R} $ as $\phi(\alpha) = f(\theta_t+\alpha p_t)$. We can have $p_t$ as a descent direction if $\phi’(0) &lt; 0$.</p> \[\phi'(0) = p_t^T \nabla f(\theta_t)\] <p>So if we choose $p_t = - \nabla f(\theta_t)$, then $p_t^T \nabla f(\theta_t) = - \nabla^2f(\theta_t) &lt; 0$ is a steepest descent direction.</p> <p><br></p> <h3 id="sgd-algorithm">SGD Algorithm</h3> <p>SGD algorithm is like a variant of GD algorithm, which targeted at the stochastic objective function or large scale machine learning problem. The only difference is the update rule at each iteration. In GD, the update rule at each iteration is:</p> \[\theta_{t+1} = \theta_t - \alpha \nabla f(\theta_t)\] <p>While in SGD, the update rule is:</p> \[\theta_{t+1} = \theta_t - \alpha \nabla f_{i_t}(\theta)\] <p>$\nabla f_{i_t}(\theta_t)$ represents stochastic gradient instead of the full gradient. Let’s illustrate this firstly. Let’s use machine learning loss function as an example. Suppose we are trying to minimize the loss function of machine learning:</p> \[\min_{\theta} \frac{1}{n} \sum_{i=1}^{n} f_i(\theta)\] <p>n could be super large. So in this case, the full gradient of objective function at $\theta$ is: $\frac{1}{n} \sum_{i=1}^{n} \nabla f_i(\theta)$. The computational cost is large when n is large since we need to calculate the full gradient in every iteration.</p> <p><br></p> <h3 id="gd-with-momentum">GD with momentum</h3> <p>To the best of my knowledge, <a class="citation" href="#rumelhart1986general">(Rumelhart et al., 1986)</a> is the first to propose adding momentum term in gradient descent algorithm and have proved that it increased the convergence rate dramatically. The update rule at each iteration is like this:</p> \[m_t = -\alpha \nabla f(\theta_t) + \beta m_{t-1}\] \[\theta_{t+1} = \theta_t + m_t\] <p>Here, $\alpha$ is the step size, or called learning rate. $\beta$ is the momentum parameter. $m_t$ is the modification on $\theta$ at $t$ iteration. The motification of $\theta$ at current iteration depends on both the current gradient and the $\theta$ change at previous iterations. Intuitively, the rationale for the use of the momentum term is that the steepest descent is particularly slow when there is a long and narrow valley in the error function surface. In this situation, the direction of the gradient is almost perpendicular to the long axis of the valley. The system thus oscillates back and forth in the direction of the short axis, and only moves very slowly along the long axis of the valley. The momentum term helps average out the oscillation along the short axis while at the same time adds up contributions along the long axis <a class="citation" href="#rumelhart1986general">(Rumelhart et al., 1986)</a>.</p> <p>To better understand the intuition behind the momentum term, Rauf Bhat provides a clear illustration in his article on <a href="https://towardsdatascience.com/gradient-descent-with-momentum-59420f626c8f" rel="external nofollow noopener" target="_blank">gradient descent with momentum</a>.</p> <p>In this update rule, $\alpha$ and $\beta$ are hyperparameters. When $\alpha = 1-\beta$, $m_t$ represents the exponential moving average. Intuitively, we assign a larger weight to the gradient from a more recent iteration. This approach makes sense because gradients are typically more similar when their corresponding points are closer together.</p> <p><br></p> <h3 id="sgd-with-momentum">SGD with momentum</h3> <p><br></p> <h3 id="adagrad">AdaGrad</h3> <p><br></p> <h3 id="rmsprop">RMSProp</h3> <p><br></p> <h3 id="adam">Adam</h3> </div> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">1986</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="rumelhart1986general" class="col-sm-8"> <div class="title">A general framework for parallel distributed processing</div> <div class="author"> David E Rumelhart, Geoffrey E Hinton, James L McClelland, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? ' others' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Parallel distributed processing: Explorations in the microstructure of cognition</em>, 1986 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Jinhua Lyu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 20, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?06cae41083477f121be8cd9797ad8e2f"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0}};</script> <script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.js" integrity="sha256-aVkDxqyzrB+ExUsOY9PdyelkDhn/DfrjWu08aVpqNlo=" crossorigin="anonymous"></script> <script>document.addEventListener("readystatechange",()=>{"complete"===document.readyState&&document.querySelectorAll("pre>code.language-pseudocode").forEach(e=>{const t=e.textContent,d=e.parentElement.parentElement;let n=document.createElement("pre");n.classList.add("pseudocode");const o=document.createTextNode(t);n.appendChild(o),d.appendChild(n),d.removeChild(e.parentElement),pseudocode.renderElement(n)})});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-stochastic-gradient-descent-sgd-with-momentum-adagrad-rmsprop-and-adam-algorithms",title:"Stochastic Gradient Descent, SGD with momentum, AdaGrad, RMSProp, and Adam algorithms",description:"A discussion on different stochastic gradient algorithms and why they are efficient",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD-algorithms/"}},{id:"news-graduated-from-ut-austin-orie",title:"Graduated from UT-Austin ORIE! \ud83c\udf89",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6A%69%6E%68%75%61%6C%79%75%32%30%32%38@%75.%6E%6F%72%74%68%77%65%73%74%65%72%6E.%65%64%75","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/JinhuaLyu","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/jinhua-lyu","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>